{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a421e620-898b-417b-aaa1-61b4577c88d5",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Imports et préparations\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e9dbdf6-eef9-49cc-9b63-bd6cf2640033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f65a73-cac4-4c21-b09a-d182ab9d6e15",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 1. Chargement et nettoyage\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f360a56-13b2-4c4b-a77a-4d19afefcca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/articles3_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80e84986-76c3-40f0-ae8e-0bdd56d3e919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Bank of England expected to leave interest rat...</td>\n",
       "      <td>The Bank of England’s policymakers may not be ...</td>\n",
       "      <td>bank england policymakers may united today pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Federal Reserve holds interest rates, defying ...</td>\n",
       "      <td>The US Federal Reserve kept interest rates on ...</td>\n",
       "      <td>federal reserve kept interest rate hold signal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>John Lewis tells some head office staff to wor...</td>\n",
       "      <td>John Lewis is asking some head office staff to...</td>\n",
       "      <td>john lewis asking head office staff spend leas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>What could Albanese do to improve productivity...</td>\n",
       "      <td>In his address last week at the National Press...</td>\n",
       "      <td>address last week national press club prime mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>EU accuses China’s AliExpress of ‘systemic fai...</td>\n",
       "      <td>The European Commission has accused the online...</td>\n",
       "      <td>european commission accused online retailer al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    section                                              title  \\\n",
       "0  Business  Bank of England expected to leave interest rat...   \n",
       "1  Business  Federal Reserve holds interest rates, defying ...   \n",
       "2  Business  John Lewis tells some head office staff to wor...   \n",
       "3  Business  What could Albanese do to improve productivity...   \n",
       "4  Business  EU accuses China’s AliExpress of ‘systemic fai...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Bank of England’s policymakers may not be ...   \n",
       "1  The US Federal Reserve kept interest rates on ...   \n",
       "2  John Lewis is asking some head office staff to...   \n",
       "3  In his address last week at the National Press...   \n",
       "4  The European Commission has accused the online...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  bank england policymakers may united today pro...  \n",
       "1  federal reserve kept interest rate hold signal...  \n",
       "2  john lewis asking head office staff spend leas...  \n",
       "3  address last week national press club prime mi...  \n",
       "4  european commission accused online retailer al...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd365a4-c7af-40ca-add9-5cea56b8b43f",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 2. Vectorisation TF-IDF\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc69d1e3-e89a-4a2c-a07f-cf95f6699907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF-IDF shape : (60000, 7500)\n"
     ]
    }
   ],
   "source": [
    "texts_cleaned = df['text_cleaned'].astype(str).tolist()\n",
    "\n",
    "# Transformation des textes en vecteurs via TF-IDF\n",
    "# Chaque texte devient un vecteur de dimension 7500\n",
    "# Chaque dimension correspond à un mot important (en anglais ici)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=7500)\n",
    "X_tfidf = vectorizer.fit_transform(texts_cleaned).toarray()\n",
    "print(f\"[INFO] TF-IDF shape : {X_tfidf.shape}\")  # ex : (n_documents, 7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72eceeb-a3fc-42d3-a88f-0229aba276e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalisation L1 des vecteurs TF-IDF\n",
    "X_tfidf = normalize(X_tfidf, norm='l2')\n",
    "\n",
    "# Conversion vers un tenseur PyTorch (utile pour l'entraînement)\n",
    "X_tensor = torch.tensor(X_tfidf, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c51ad-3f3e-404a-aa5f-b2954f8ba8b7",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 3. Recurrent Neural Networks en PyTorch\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd73d39-6465-499e-9e58-3a5b19e5099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "\n",
    "# 1. Tokeniser les textes\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "texts_cleaned = df[\"text_cleaned\"].astype(str).tolist()\n",
    "tokenized = [tokenizer(text) for text in texts_cleaned]\n",
    "\n",
    "# 2. Construire le Counter\n",
    "counter = Counter()\n",
    "for tokens in tokenized:\n",
    "    counter.update(tokens)\n",
    "\n",
    "# 3. Ajouter les tokens spéciaux à la main\n",
    "vocab_tokens = [\"<pad>\", \"<unk>\"] + [token for token, freq in counter.items()]\n",
    "vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
    "\n",
    "# 4. Fonction pour indexer les tokens\n",
    "def encode(tokens, vocab, unk_token=\"<unk>\"):\n",
    "    return [vocab.get(token, vocab[unk_token]) for token in tokens]\n",
    "\n",
    "# 5. Conversion des textes en séquences d'indices\n",
    "indexed = [encode(tokens, vocab) for tokens in tokenized]\n",
    "\n",
    "# 6. Padding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tensor_sequences = [torch.tensor(seq, dtype=torch.long) for seq in indexed]\n",
    "padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08935004-2c3e-41c4-af11-3888d2b223a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(df[\"section\"])\n",
    "labels_tensor = torch.tensor(labels_encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85dc4428-f403-4dd7-9ce3-335d0a00f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "# Vérification\n",
    "assert padded_sequences.size(0) == labels_tensor.size(0), \"Mismatch entre données et labels\"\n",
    "\n",
    "# Création du dataset complet\n",
    "full_dataset = TensorDataset(padded_sequences, labels_tensor)\n",
    "\n",
    "# Taille du split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split aléatoire\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cce6e601-f36e-47dc-91b9-5686f6792638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = nn.Parameter(torch.randn(embed_dim, hidden_size))\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len) avec indices de tokens\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, _ = embedded.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = embedded[:, t, :]  # (batch_size, embed_dim)\n",
    "            h_t = torch.tanh(x_t @ self.W_xh + h_t @ self.W_hh + self.b_h)  # (batch_size, hidden_size)\n",
    "        \n",
    "        output = self.fc(h_t)  # (batch_size, output_size)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827a8dd-87b0-48c5-b09d-3dabacf7e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "\n",
    "model = CustomRNN(vocab_size, embed_dim, hidden_dim, output_dim, pad_idx)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.long()  # indices tokens\n",
    "        batch_y = batch_y.long()  # classes\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)  # (batch_size, output_dim)\n",
    "        \n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3dd102-8079-4360-8b0c-234ad381f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.long()\n",
    "        batch_y = batch_y.long()\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        preds = outputs.argmax(dim=1)  # classes prédites\n",
    "        \n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "print(f\"Test accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74004475-dac1-47e6-88b0-1b7d72db059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 — Loss: 0.9152\n",
      "Epoch 10 — Loss: 0.0865\n",
      "Epoch 20 — Loss: 0.0098\n",
      "Epoch 30 — Loss: 0.0015\n",
      "Epoch 40 — Loss: 0.0004\n",
      "Epoch 50 — Loss: 0.0001\n",
      "Epoch 60 — Loss: 0.0000\n",
      "Epoch 70 — Loss: 0.0000\n",
      "Epoch 80 — Loss: 0.0000\n",
      "Epoch 90 — Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "\n",
    "model = CustomRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "X = torch.randn(5, 7, input_size)  # (batch_size=5, seq_len=7, input_size=10)\n",
    "Y = torch.randn(5, output_size)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} — Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cd9ea-ebe7-4716-a7ad-2118b932a569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
