# ğŸ” Analyse SÃ©mantique de Textes â€” Embeddings & SimilaritÃ©

Ce module Python explore diffÃ©rentes **reprÃ©sentations vectorielles de textes** (embeddings) pour analyser la **proximitÃ© sÃ©mantique** entre des mots ou des documents Ã  partir dâ€™un corpus. Il combine des approches classiques (BoW, TF-IDF) et avancÃ©es (Word2Vec, BERT).

---

## ğŸ¯ Objectifs

- ReprÃ©senter les textes sous forme numÃ©rique (vectorielle).
- Ã‰tudier la frÃ©quence et lâ€™importance des mots avec **BoW** et **TF-IDF**.
- Apprendre des relations sÃ©mantiques avec **Word2Vec**.
- Extraire des reprÃ©sentations contextuelles puissantes avec **BERT**.
- Visualiser les similaritÃ©s entre mots et documents via **cosine similarity**, **heatmaps**, et **t-SNE**.

---

## ğŸ“¦ Contenu du Code

### 1. ğŸ”¡ BoW & TF-IDF

#### â¤ Pourquoi ?
- Analyser les mots prÃ©sents dans chaque article.
- Mesurer leur importance relative.

#### â¤ Ã‰tapes :
- Charger les textes nettoyÃ©s (`cleaned_text`) depuis un DataFrame.
- CrÃ©er des reprÃ©sentations BoW (`CountVectorizer`) et TF-IDF (`TfidfVectorizer`).
- Afficher les matrices sous forme de `DataFrame`.
- Calculer manuellement le TF, IDF et TF-IDF dâ€™un mot spÃ©cifique pour comparer avec `sklearn`.

---

### 2. ğŸ§  Word2Vec (Gensim)

#### â¤ Pourquoi ?
- Apprendre les relations sÃ©mantiques entre les mots (proximitÃ©, analogie, etc.).
- DÃ©couvrir les mots proches dans un espace vectoriel appris Ã  partir du corpus.

#### â¤ Ã‰tapes :
- Tokeniser chaque article en liste de mots.
- EntraÃ®ner un modÃ¨le `Word2Vec` sur le corpus.
- Visualiser les similaritÃ©s entre paires de mots.
- Extraire les mots les plus proches de certains mots-clÃ©s.
- Projeter les vecteurs appris dans un plan 2D avec `t-SNE`.

---

### 3. ğŸ¤– BERT Multilingue (`bert-base-multilingual-cased`)

#### â¤ Pourquoi ?
- Capturer le **contexte** autour de chaque mot dans une phrase.
- Obtenir des embeddings de qualitÃ© supÃ©rieure, sensibles Ã  la position et au sens du mot.

#### â¤ Ã‰tapes :
- Charger le modÃ¨le et le tokenizer BERT via `transformers`.
- Pour chaque mot cible, extraire son embedding dans chaque article.
- Calculer toutes les similaritÃ©s cosinus entre paires de mots et dâ€™articles.
- Organiser les rÃ©sultats dans une `DataFrame` (pivotÃ©e ou plate).
- Moyenne des embeddings par mot pour visualisation globale.
- Visualiser les relations par **heatmap** et **t-SNE**.

---

## ğŸ“Š Visualisations

- **Matrice de similaritÃ© (heatmap)** : montre quels mots sont les plus proches en moyenne.
- **Projection t-SNE** : rend visible la structure sÃ©mantique dans un plan 2D.

---

## ğŸ› ï¸ Comment utiliser ce script

### 1. PrÃ©requis

Installez les bibliothÃ¨ques nÃ©cessaires :

```bash
pip install pandas numpy sklearn gensim seaborn matplotlib transformers torch
```
  
### 2. ğŸ“‚ PrÃ©parez votre corpus

Le script attend un DataFrame nommÃ© `df_sample` contenant au minimum les colonnes suivantes :

- `cleaned_text` : le texte nettoyÃ© de chaque document (minimisation, tokenisation, etc.)
- `section` : un identifiant, numÃ©ro ou titre associÃ© au document (utile pour indexer les rÃ©sultats)

**Exemple :**

```python
import pandas as pd

df_sample = pd.DataFrame({
   "section": ["Article 1", "Article 2"],
   "cleaned_text": [
      "iran signed trade agreement with china",
      "rugby world cup attracts millions of viewers"
   ]
})
```

### 3. ğŸ¯ Modifiez les mots cibles (optionnel)

Vous pouvez personnaliser la liste des mots Ã  analyser avec Word2Vec et BERT en modifiant la variable :

```python
target_words = ["rugby", "citizen", "arab", "emirate", "signed", "trade", "village", "songwriter", "thunder", "nba", "fertility", "maker", "report"]
```

Ces mots seront utilisÃ©s pour :

- Extraire leurs vecteurs d'embedding dans diffÃ©rents contextes
- Comparer leur similaritÃ© avec d'autres mots
- Visualiser leur position dans un espace rÃ©duit (t-SNE)

---

## ğŸ§ª RÃ©sultats observables

Le script compare les scores de similaritÃ© sÃ©mantique produits par diffÃ©rentes mÃ©thodes :

- **BoW** : sensible Ã  la frÃ©quence brute (prÃ©sence de mots)
- **TF-IDF** : pondÃ¨re par raretÃ© du mot (informativitÃ©)
- **Word2Vec** : apprend des relations sÃ©mantiques basÃ©es sur le contexte global
- **BERT** : capture la signification contextuelle exacte dans chaque phrase

---

## ğŸ“ Exemple de sortie

### ğŸ”¥ Heatmap de similaritÃ© moyenne BERT

Affiche les similaritÃ©s moyennes entre les mots cibles :

```
Cosine Similarity Between Words (Average Embeddings)
```

### ğŸ“ Comparaison TF-IDF manuel vs sklearn

Affiche pour un mot donnÃ© les calculs de :

```
TF-IDF (manuel) = 0.212
TF-IDF (sklearn) = 0.213
```

### ğŸ”— Word2Vec

Mesure de similaritÃ© entre mots (apprise depuis le corpus) :

```
'arab' â†” 'emirate' : 0.842
```

### ğŸ§  Visualisation t-SNE de BERT

RÃ©duction de dimension 2D des embeddings BERT :

```
ğŸ§  t-SNE â€“ BERT Embeddings par mot et article
```

---

## ğŸ“Œ Ã€ personnaliser

- **Corpus** : Ajoutez ou remplacez vos propres documents dans `df_sample`
- **Word2Vec** : Changez les paramÃ¨tres `vector_size`, `window`, `min_count`, etc.
- **TF-IDF** : Ajoutez des `stop_words`, modifiez `ngram_range` ou `max_df`
- **BERT** : Testez dâ€™autres modÃ¨les comme `bert-base-uncased`, `camembert-base`, etc.

---

## ğŸ“¬ Contact

Pour toute question ou suggestion d'amÃ©lioration, n'hÃ©sitez pas Ã  me contacter ou ouvrir une issue si ce projet est intÃ©grÃ© dans un dÃ©pÃ´t GitHub.

---

## ğŸ¤” Pourquoi privilÃ©gier BERT pour la classification dâ€™articles de presse ?

Pour un projet de **classification dâ€™articles de presse selon leur contenu**, il est gÃ©nÃ©ralement plus pertinent dâ€™utiliser **BERT** plutÃ´t que des mÃ©thodes classiques comme BoW, TF-IDF ou mÃªme Word2Vec. 

**Pourquoi ?**

- **ComprÃ©hension contextuelle** : BERT prend en compte le contexte complet de chaque mot dans la phrase, ce qui permet de mieux saisir la signification rÃ©elle des textes journalistiques, souvent riches et nuancÃ©s.
- **Gestion des ambiguÃ¯tÃ©s** : Les articles de presse contiennent frÃ©quemment des mots polysÃ©miques (ayant plusieurs sens possibles) ou ambigus ; BERT adapte lâ€™embedding de chaque mot Ã  son contexte prÃ©cis.
- **Performances supÃ©rieures** : Les modÃ¨les basÃ©s sur BERT atteignent gÃ©nÃ©ralement de meilleurs scores de classification sur des tÃ¢ches de comprÃ©hension de texte, car ils capturent des relations sÃ©mantiques complexes.
- **Multilinguisme** : Avec des variantes comme `bert-base-multilingual-cased`, il est possible de traiter des articles dans plusieurs langues sans entraÃ®ner un modÃ¨le distinct pour chaque langue.

En rÃ©sumÃ©, **BERT** offre des reprÃ©sentations plus riches et adaptÃ©es Ã  la complexitÃ© des articles de presse, ce qui amÃ©liore la prÃ©cision de la classification par rapport aux approches traditionnelles.
