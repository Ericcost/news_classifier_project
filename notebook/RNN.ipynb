{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3555a5a-9aec-4ea2-9261-a16257dbdac6",
   "metadata": {},
   "source": [
    "1/ Objectif \n",
    "Vous devrez réaliser un modèle de classification de texte à l'aide d'un réseau de neurones récurent (RNN), sans utiliser de module préfabriqué (nn.RNN). Le modèle devra prédire un label (ex. positif, négatif ou bien neutre) en fonction de votre jeux de donnée. Attention vous aurez besoin d'un jeux de donnée permettant un apprentissage supervisé.\n",
    " \n",
    "2/ Etape dans la construction du modèle \n",
    "Prétraitement des données : vous avez déjà vu cette partie, je n'ai pas besoin de vous la redétailler ; \n",
    "Architecture du modèle : vous devrez implémenter un RNN \"from scratch\" \n",
    "Une couche d'embedding (nn.Embedding) pour transformer les indices en vecteurs. \n",
    "Un bloc RNN, ou l'état caché (hidden_state) est mis à jour de façon récursive (vous avez la formule pour rappel dans le ppt). \n",
    "Une couche linéaire finale pour produire votre prédiction à partir du dernier état caché. \n",
    "Optionnel (si tout fonctionne) : dropout, batchnorm, etc.. \n",
    "\n",
    "3/ Entraînement \n",
    "Attention, il va falloir modifier la fonction de perte habituel (MSELoss) pour ce cas d'application ! Suivez les métriques de précision sur le jeu de validation (ou de test). Afficher les métriques toutes les X époques. Enfin, je conseille d'afficher un graphique avec Matplotlib pour vérifier l'entraînement, nottament, la courbe d'apprentissage vs la courbe de validation (ou test). \n",
    " \n",
    "4/ Mes conseils\n",
    "Commencez simple. Une seule couche RNN suffit.\n",
    "Testez d'abord sur un petit sous-ensemble pour valider le code.\n",
    "Gardez une longueur fixe pour les séquences (padding).\n",
    "Vérifiez vos dimensions avec .shape à chaque étape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded789a-5f16-4e66-9997-67da67de5c74",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Imports et préparations\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139fc83e-adc9-48ba-be05-6a26d29f599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from kagglehub) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests->kagglehub) (2025.6.15)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f36f46-15a6-4ad1-88c1-e155255bf960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ca381-41eb-44eb-ba08-49497bfcc09b",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 1. Chargement et nettoyage\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a1be2f-f52f-49c5-a50d-8aac0261f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ericcosterousse/.cache/kagglehub/datasets/abdelmalekeladjelet/sentiment-analysis-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"abdelmalekeladjelet/sentiment-analysis-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ecf4bc3-240a-4d82-a0ec-32afbcd3d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du dossier:\n",
      "['sentiment_data.csv']\n"
     ]
    }
   ],
   "source": [
    "# Liste les fichiers dans ce dossier\n",
    "print(\"Contenu du dossier:\")\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c7bb7c-900f-4bd6-95e4-6842ff2229d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path, 'sentiment_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1341a913-132c-45d1-b142-00a88158ebb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lets forget apple pay required brand new iphon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nz retailers don’t even contactless credit car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>forever acknowledge channel help lessons ideas...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>whenever go place doesn’t take apple pay doesn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>apple pay convenient secure easy use used kore...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Comment  Sentiment\n",
       "0           0  lets forget apple pay required brand new iphon...          1\n",
       "1           1  nz retailers don’t even contactless credit car...          0\n",
       "2           2  forever acknowledge channel help lessons ideas...          2\n",
       "3           3  whenever go place doesn’t take apple pay doesn...          0\n",
       "4           4  apple pay convenient secure easy use used kore...          2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badbca7b-7a6a-422c-bb94-44cdb4ac9b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241145, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307922bd-74fd-4e34-af04-b93461f7f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b995ce5b-c25e-4bb0-bc78-2d363f7b00b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets forget apple pay required brand new iphon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nz retailers don’t even contactless credit car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forever acknowledge channel help lessons ideas...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever go place doesn’t take apple pay doesn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay convenient secure easy use used kore...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Sentiment\n",
       "0  lets forget apple pay required brand new iphon...          1\n",
       "1  nz retailers don’t even contactless credit car...          0\n",
       "2  forever acknowledge channel help lessons ideas...          2\n",
       "3  whenever go place doesn’t take apple pay doesn...          0\n",
       "4  apple pay convenient secure easy use used kore...          2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54214937-e4a8-4b0b-bc48-ba3076bcc07c",
   "metadata": {},
   "source": [
    "# Pourquoi prétraiter les données ?\n",
    "Le texte brut n’est pas directement exploitable par un réseau de neurones. Il faut le transformer en une forme numérique compréhensible par le modèle, tout en gardant le maximum d’information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bad7be-ceb3-40bb-90a7-349f92987d10",
   "metadata": {},
   "source": [
    "### 1. Nettoyage du texte (optionnel mais recommandé)\n",
    "Objectif : enlever le bruit (URLs, ponctuations inutiles, mentions @, hashtags non pertinents, caractères spéciaux, etc.)\n",
    "\n",
    "Pourquoi ? Ces éléments peuvent nuire à la qualité des embeddings et du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884afbb3-0393-4627-bb8a-f4aaa87c80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.update(['u', 'us', 'q'])\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Nettoyage du texte : minuscules, suppression HTML, ponctuation, chiffres, espaces multiples.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        text = text.lower()\n",
    "        # Normalisation unicode (accents etc.)\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "        # Supprimer les URLs (http, https, www)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "        # Supprimer les emails\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "        # Supprimer ponctuation et chiffres, garder lettres et espaces uniquement\n",
    "        # (on enlève aussi les acronymes avec points en une fois)\n",
    "        text = re.sub(r'\\b[a-z]\\.', '', text)  # enlever lettres suivies d'un point (ex: u.)\n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)  # garder lettres et espaces uniquement\n",
    "    \n",
    "        # Enlever espaces multiples\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenisation simple par split des mots.\n",
    "        \"\"\"\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"\n",
    "        Suppression des stopwords.\n",
    "        \"\"\"\n",
    "        filtered = [token for token in tokens if token not in self.stop_words]  \n",
    "        return filtered\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        \"\"\"\n",
    "        Lemmatisation des tokens.\n",
    "        \"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Pipeline complet combinant toutes les étapes\n",
    "        \"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        no_stop = self.remove_stopwords(tokens)\n",
    "        lemmas = self.lemmatize(no_stop)\n",
    "        return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc6be02-fec5-4440-a4e0-257768d4c6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love new policy government check\n"
     ]
    }
   ],
   "source": [
    "prep = Preprocessing()\n",
    "tweet = \"I love the new policy from the government! Check http://example.com\"\n",
    "processed = prep.preprocess(tweet)\n",
    "print(processed)\n",
    "# Résultat possible : \"love new policy check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efac04a4-571e-45f7-b2d2-4acb93a039d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocessing()\n",
    "df['Cleaned_Comment'] = df['Comment'].astype(str).apply(preprocess.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b8d58c-2d68-46fd-aa5f-62f9a60cffa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Cleaned_Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets forget apple pay required brand new iphon...</td>\n",
       "      <td>1</td>\n",
       "      <td>let forget apple pay required brand new iphone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nz retailers don’t even contactless credit car...</td>\n",
       "      <td>0</td>\n",
       "      <td>nz retailer dont even contactless credit card ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forever acknowledge channel help lessons ideas...</td>\n",
       "      <td>2</td>\n",
       "      <td>forever acknowledge channel help lesson idea e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever go place doesn’t take apple pay doesn...</td>\n",
       "      <td>0</td>\n",
       "      <td>whenever go place doesnt take apple pay doesnt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay convenient secure easy use used kore...</td>\n",
       "      <td>2</td>\n",
       "      <td>apple pay convenient secure easy use used kore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Sentiment  \\\n",
       "0  lets forget apple pay required brand new iphon...          1   \n",
       "1  nz retailers don’t even contactless credit car...          0   \n",
       "2  forever acknowledge channel help lessons ideas...          2   \n",
       "3  whenever go place doesn’t take apple pay doesn...          0   \n",
       "4  apple pay convenient secure easy use used kore...          2   \n",
       "\n",
       "                                     Cleaned_Comment  \n",
       "0  let forget apple pay required brand new iphone...  \n",
       "1  nz retailer dont even contactless credit card ...  \n",
       "2  forever acknowledge channel help lesson idea e...  \n",
       "3  whenever go place doesnt take apple pay doesnt...  \n",
       "4  apple pay convenient secure easy use used kore...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "169d19da-5c2b-4e5b-aaca-a9e96a4b3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Chaque ligne est déjà un string \"token1 token2 token3 ...\"\n",
    "tokenized_texts = [text.split() for text in df['Cleaned_Comment']]\n",
    "\n",
    "# Construire le vocabulaire (compter les mots)\n",
    "word_counts = Counter()\n",
    "for tokens in tokenized_texts:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "# Limiter la taille du vocabulaire (tu peux ajuster)\n",
    "vocab_size = 5000\n",
    "\n",
    "most_common = word_counts.most_common(vocab_size - 2)  # -2 pour <PAD> et <UNK>\n",
    "\n",
    "# Mapping mot → index\n",
    "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for i, (word, _) in enumerate(most_common, start=2):\n",
    "    word2idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43579e45-643c-496d-9f7e-af8a1741cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50  # longueur fixe de séquence\n",
    "\n",
    "def encode_and_pad(tokens):\n",
    "    encoded = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
    "    if len(encoded) > max_len:\n",
    "        return encoded[:max_len]\n",
    "    else:\n",
    "        return encoded + [word2idx['<PAD>']] * (max_len - len(encoded))\n",
    "\n",
    "encoded_texts = [encode_and_pad(text.split()) for text in df['Cleaned_Comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e3414a-4d76-445a-a3fc-9f4c1333bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['Sentiment'].values  # 0,1,2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65bd352b-2f74-4093-8862-4f657e1c7119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3100, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3155, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3367, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3612, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3672, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/5w/jbysldxx6d32vym0rw5yrssm0000gn/T/ipykernel_4964/4241869917.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/ericcosterousse/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encoded_texts, labels):\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encoded_texts[idx], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ca9cc5d-f673-4d77-abf6-2db025db1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9d9ed-125a-4f35-b4f2-c46c256c9910",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Embeddings, Normalisation & RNN\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e13d1186-3ca2-4bd7-a9e4-2875ccdf2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNFromScratch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Matrices de poids\n",
    "        self.W_ih = nn.Linear(embedding_dim, hidden_dim, bias=True)\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: LongTensor (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.layer_norm(embedded)\n",
    "        \n",
    "        # Initialisation état caché h_0 = 0\n",
    "        h_t = torch.zeros(batch_size, self.W_hh.out_features, device=x.device)\n",
    "        \n",
    "        # Boucle temporelle\n",
    "        for t in range(seq_len):\n",
    "            x_t = embedded[:, t, :]  # (batch_size, embedding_dim)\n",
    "            h_t = torch.tanh(self.W_ih(x_t) + self.W_hh(h_t))\n",
    "        \n",
    "        # Prédiction à partir du dernier état caché\n",
    "        out = self.output_layer(h_t)  # (batch_size, output_dim)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1fcc9a-0813-467c-bbe6-252f15cd3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = RNNFromScratch(vocab_size=5000, embedding_dim=100, hidden_dim=64, output_dim=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d8ac8e9-610f-4fc0-804a-1642f29107f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.0682 | Accuracy: 0.4270\n",
      "Epoch 2 | Loss: 1.0677 | Accuracy: 0.4276\n",
      "Epoch 3 | Loss: 1.0676 | Accuracy: 0.4276\n",
      "Epoch 4 | Loss: 1.0675 | Accuracy: 0.4277\n",
      "Epoch 5 | Loss: 1.0675 | Accuracy: 0.4278\n",
      "Epoch 6 | Loss: 1.0674 | Accuracy: 0.4278\n",
      "Epoch 7 | Loss: 1.0674 | Accuracy: 0.4279\n",
      "Epoch 8 | Loss: 1.0674 | Accuracy: 0.4279\n",
      "Epoch 9 | Loss: 1.0674 | Accuracy: 0.4279\n",
      "Epoch 10 | Loss: 1.0673 | Accuracy: 0.4279\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RNNFromScratch(vocab_size, embedding_dim=100, hidden_dim=64, output_dim=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a282e083-8426-4264-a167-c0170798e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4284\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8722fd-7bd9-4866-aa93-864b10036e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
